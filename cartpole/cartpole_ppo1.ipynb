{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the open AI gym baseline with some slight modifications. Mostly copied code from here\n",
    "https://github.com/openai/baselines/tree/master/baselines/ppo1. Looks like the version I got from pip and the version currently on master don't quite sync up. (For example there is no tf_util.save_state fcn so we save manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /var/folders/qq/gpxz4l6s1tndfdhysbz8bdym0000gn/T/openai-2018-12-03-18-55-50-897819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sgillen/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from baselines.common.cmd_util import make_mujoco_env, mujoco_arg_parser\n",
    "from baselines.common import tf_util as U\n",
    "import tensorflow as tf\n",
    "from baselines import logger\n",
    "import os\n",
    "import sys\n",
    "from baselines.ppo1 import mlp_policy, pposgd_simple\n",
    "\n",
    "import gym\n",
    "import gym_ucsb_robolab\n",
    "import policies.mlp_relu_policy as mlp_relu_policy\n",
    "\n",
    "import numpy as np\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "#Needed for saving \n",
    "import errno, datetime, time, inspect\n",
    "\n",
    "def train(env_id, num_timesteps, seed=0):\n",
    "\n",
    "    U.make_session(num_cpu=16).__enter__()\n",
    "    \n",
    "    def policy_fn(name, ob_space, ac_space):\n",
    "        #return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space, hid_size=64, num_hid_layers=64)\n",
    "        return mlp_relu_policy.ReluMlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space, hid_size=64, num_hid_layers=4)\n",
    "\n",
    "    env = gym.make(env_id)\n",
    "    pi = pposgd_simple.learn(env, policy_fn,\n",
    "            max_timesteps=num_timesteps,\n",
    "            timesteps_per_actorbatch=2048,\n",
    "            clip_param=0.2, entcoeff=0.0,\n",
    "            optim_epochs=10, optim_stepsize=3e-4, optim_batchsize=64,\n",
    "            gamma=0.99, lam=0.95, schedule='linear',\n",
    "        )\n",
    "    env.close()\n",
    "   \n",
    "    return pi\n",
    "\n",
    "\n",
    "\n",
    "def save_results(filename, description = None):\n",
    "    \"\"\" \n",
    "    description: saves the results of a run of the second cell (the one that calls train) in this notebook\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    save_dir = \"data/\" + filename + \"/\"\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "    if description is None:\n",
    "        description  = input(\"please enter a description of the run\")\n",
    "        \n",
    "    datetime_str = str(datetime.datetime.today())\n",
    "    datetime_str = datetime_str.replace(\" \", \"_\")\n",
    "    \n",
    "    runtime_str = str(datetime.timedelta(seconds = runtime))\n",
    "    \n",
    "    readme = open(save_dir + \"README.txt\", \"w+\")\n",
    "    readme.write(\"datetime: \" + datetime_str + \"\\n\\n\")\n",
    "    readme.write(\"enviroment: \" + env_name + \"\\n\\n\")\n",
    "    readme.write(\"description: \" + description + \"\\n\\n\")\n",
    "    readme.write(\"time_elapsed: \" + runtime_str + \"\\n\\n\")\n",
    "    readme.write(\"num_timesteps: \" + str(num_timesteps) + \"\\n\\n\")\n",
    "    readme.write(\"seed: \" + str(seed) + \"\\n\\n\")\n",
    "    readme.close()\n",
    "\n",
    "    # TODO add code snippets that correspond to the run\n",
    "    # TODO somehow store the tensorboard logs here after the fact\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(tf.get_default_session(), save_dir + filename)\n",
    "    \n",
    "    os.rename(\"./tmp_logs/\", save_dir + \"tensorboard\")\n",
    "   \n",
    "   \n",
    "#env_name = \"Acrobot-v1\"\n",
    "env_name = \"InvertedPendulum-v2\"\n",
    "#env_name = 'InvertedPendulumPyBulletEnv-v0'\n",
    "#env_name = \"su_cartpole_et-v0\"\n",
    "#env_name = \"InvertedDoublePendulum-v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "********** Iteration 0 ************\n",
      "Optimizing...\n",
      "     pol_surr |    pol_entpen |       vf_loss |            kl |           ent\n",
      "     -0.00565 |       0.00000 |      30.46045 |       0.00060 |       1.41631\n",
      "     -0.02725 |       0.00000 |      27.36079 |       0.01377 |       1.41056\n",
      "     -0.03244 |       0.00000 |      20.03138 |       0.02617 |       1.40630\n",
      "     -0.03389 |       0.00000 |      10.06302 |       0.03100 |       1.40022\n",
      "     -0.03495 |       0.00000 |       7.24170 |       0.02884 |       1.39632\n",
      "     -0.03585 |       0.00000 |       6.79682 |       0.02808 |       1.39207\n",
      "     -0.03652 |       0.00000 |       6.49754 |       0.02955 |       1.38842\n",
      "     -0.03729 |       0.00000 |       6.30366 |       0.03028 |       1.38445\n",
      "     -0.03807 |       0.00000 |       6.27583 |       0.03147 |       1.38186\n",
      "     -0.03882 |       0.00000 |       6.23063 |       0.03241 |       1.37765\n",
      "Evaluating losses...\n",
      "     -0.03968 |       0.00000 |       6.16091 |       0.03026 |       1.37564\n",
      "---------------------------------\n",
      "| EpisodesSoFar   | 251         |\n",
      "| EpLenMean       | 8.06        |\n",
      "| EpRewMean       | 8.06        |\n",
      "| EpThisIter      | 251         |\n",
      "| ev_tdlam_before | 0.00259     |\n",
      "| loss_ent        | 1.3756423   |\n",
      "| loss_kl         | 0.030260162 |\n",
      "| loss_pol_entpen | 0.0         |\n",
      "| loss_pol_surr   | -0.03968087 |\n",
      "| loss_vf_loss    | 6.160914    |\n",
      "| TimeElapsed     | 3.7         |\n",
      "| TimestepsSoFar  | 2046        |\n",
      "---------------------------------\n",
      "load_state method is deprecated, please use load_variables instead\n",
      "INFO:tensorflow:Restoring parameters from /Users/sgillen/work_dir/ucsb/sgillen_research/cartpole/data/invertedpendulum_3layer/invertedpendulum_3layer\n"
     ]
    }
   ],
   "source": [
    "# comment one of these lines to switch between loading weights or training them from scratch\n",
    "load_pretrained_network = True\n",
    "#load_pretrained_network = False\n",
    "\n",
    "\n",
    "if load_pretrained_network: #load the weights\n",
    "    save_name = 'invertedpendulum_3layer'\n",
    "    \n",
    "    pi = train(env_name, num_timesteps=1, seed=0)\n",
    "    # TODO eventually need to switch to .load_variables() instead of U.load_state() but this didn't work by default for me\n",
    "    U.load_state(os.getcwd() + '/data/'+ save_name + '/' + save_name)\n",
    "    \n",
    "else: #learn the weight\n",
    "    num_timesteps = 1\n",
    "    seed = 0\n",
    "    \n",
    "    print(\"training\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    logger.configure(dir = \"./tmp_logs\", format_strs=[\"tensorboard\"] )\n",
    "    with tf.device(\"/cpu:0\"):    \n",
    "        pi= train(env_name, num_timesteps=num_timesteps, seed=seed)\n",
    "\n",
    "    runtime = time.time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "# Plays out a trained policy\n",
    "\n",
    "env = make_mujoco_env(env_name,seed=0)\n",
    "ob = env.reset()     \n",
    "\n",
    "while True:\n",
    "    action = pi.act(stochastic=False, ob=ob)[0]\n",
    "    ob, _, done, _ =  env.step(action)\n",
    "    #if reward == 1:\n",
    "    #    print(\"balanced\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        ob = env.reset()\n",
    "        \n",
    "#U.save_state(\"./saved/5mil_flat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_weights = pi.get_variables()\n",
    "fc1 = all_weights[4]\n",
    "fc1_weights = fc1.value()\n",
    "fc1_weights.eval() > 0 3 only positive weights contribute anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input_iter = itertools.combinations_with_replacement(range(-10,11),4)\n",
    "#input_data = np.array([np.array(x) for x in input_iter],dtype='float32')\n",
    "#output_data = np.array([pi.act(0, x)[0] for x in input_data],dtype='float32')\n",
    "\n",
    "input_iter = itertools.combinations_with_replacement(range(-10,11),2)\n",
    "input_data = np.array([np.concatenate((np.zeros(2), np.array(x))) for x in input_iter],dtype='float32')\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "regr.fit(input_data,output_data.T.flatten())\n",
    "#regr.fit(input_data,output_data)\n",
    "#regr.fit(index, output_data.T.flatten())\n",
    "\n",
    "lin_predict = regr.predict(input_data)\n",
    "#lin_predict = regr.predict(index)\n",
    "\n",
    "print(\"coefs are\", regr.coef_)\n",
    "print(\"mean sqared error:\", mean_squared_error(lin_predict, output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(output_data)\n",
    "plt.figure()\n",
    "plt.plot(lin_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = np.array([[x, 0, 0, 0] for x in range(-10,11)])\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data])\n",
    "plt.plot(input_data[:,0], output_data)\n",
    "plt.figure()\n",
    "\n",
    "input_data = np.array([[0, x, 0, 0] for x in range(-10,11)])\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data])\n",
    "plt.plot(input_data[:,1], output_data)\n",
    "plt.figure()\n",
    "\n",
    "input_data = np.array([[0, 0, x, 0] for x in range(-10,11)])\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data])\n",
    "plt.plot(input_data[:,2], output_data)\n",
    "plt.figure()\n",
    "\n",
    "input_data = np.array([[0, 0, 0, x] for x in range(-10,11)])\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data])\n",
    "plt.plot(input_data[:,3], output_data)\n",
    "plt.figure()\n",
    "\n",
    "input_data = np.array([[x, x, x, x] for x in range(-10,11)])\n",
    "output_data = np.array([pi.act(0, x)[0] for x in input_data])\n",
    "plt.plot(input_data[:,0], output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = input_data[:,2]\n",
    "y = input_data[:,3]\n",
    "z= output_data.flatten()\n",
    "z2 = lin_predict.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_trisurf(x, y, z, cmap='viridis', edgecolor='none');\n",
    "\n",
    "ax2 = plt.axes(projection='3d')\n",
    "ax2.plot_trisurf(x, y, z2, cmap='viridis', edgecolor='none');\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
