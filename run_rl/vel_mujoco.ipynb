{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.35batch/s]\n",
      "=>>>>>>>>>> EPOCH 1\n",
      "Metrics:   episode_rewards 9.060000 epoch_idx 1.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:03<00:00,  5.35batch/s]\n",
      "=>>>>>>>>>> EPOCH 2\n",
      "Metrics:   episode_rewards 28.260000 epoch_idx 2.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:03<00:00,  5.24batch/s]\n",
      "=>>>>>>>>>> EPOCH 3\n",
      "Metrics:   episode_rewards 48.070000 epoch_idx 3.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.71batch/s]\n",
      "=>>>>>>>>>> EPOCH 4\n",
      "Metrics:   episode_rewards 74.110000 epoch_idx 4.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:03<00:00,  5.04batch/s]\n",
      "=>>>>>>>>>> EPOCH 5\n",
      "Metrics:   episode_rewards 92.040000 epoch_idx 5.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:03<00:00,  5.04batch/s]\n",
      "=>>>>>>>>>> EPOCH 6\n",
      "Metrics:   episode_rewards 109.950000 epoch_idx 6.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.73batch/s]\n",
      "=>>>>>>>>>> EPOCH 7\n",
      "Metrics:   episode_rewards 56.170000 epoch_idx 7.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.43batch/s]\n",
      "=>>>>>>>>>> EPOCH 8\n",
      "Metrics:   episode_rewards 53.560000 epoch_idx 8.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.03batch/s]\n",
      "=>>>>>>>>>> EPOCH 9\n",
      "Metrics:   episode_rewards 74.540000 epoch_idx 9.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.94batch/s]\n",
      "=>>>>>>>>>> EPOCH 10\n",
      "Metrics:   episode_rewards 85.750000 epoch_idx 10.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.10batch/s]\n",
      "=>>>>>>>>>> EPOCH 11\n",
      "Metrics:   episode_rewards 101.560000 epoch_idx 11.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.93batch/s]\n",
      "=>>>>>>>>>> EPOCH 12\n",
      "Metrics:   episode_rewards 111.490000 epoch_idx 12.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.56batch/s]\n",
      "=>>>>>>>>>> EPOCH 13\n",
      "Metrics:   episode_rewards 121.730000 epoch_idx 13.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.49batch/s]\n",
      "=>>>>>>>>>> EPOCH 14\n",
      "Metrics:   episode_rewards 137.050000 epoch_idx 14.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.54batch/s]\n",
      "=>>>>>>>>>> EPOCH 15\n",
      "Metrics:   episode_rewards 72.310000 epoch_idx 15.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.51batch/s]\n",
      "=>>>>>>>>>> EPOCH 16\n",
      "Metrics:   episode_rewards 85.980000 epoch_idx 16.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.45batch/s]\n",
      "=>>>>>>>>>> EPOCH 17\n",
      "Metrics:   episode_rewards 108.750000 epoch_idx 17.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.36batch/s]\n",
      "=>>>>>>>>>> EPOCH 18\n",
      "Metrics:   episode_rewards 124.830000 epoch_idx 18.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.28batch/s]\n",
      "=>>>>>>>>>> EPOCH 19\n",
      "Metrics:   episode_rewards 92.240000 epoch_idx 19.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.28batch/s]\n",
      "=>>>>>>>>>> EPOCH 20\n",
      "Metrics:   episode_rewards 105.740000 epoch_idx 20.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.03batch/s]\n",
      "=>>>>>>>>>> EPOCH 21\n",
      "Metrics:   episode_rewards 128.580000 epoch_idx 21.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.21batch/s]\n",
      "=>>>>>>>>>> EPOCH 22\n",
      "Metrics:   episode_rewards 138.990000 epoch_idx 22.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.19batch/s]\n",
      "=>>>>>>>>>> EPOCH 23\n",
      "Metrics:   episode_rewards 157.990000 epoch_idx 23.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.11batch/s]\n",
      "=>>>>>>>>>> EPOCH 24\n",
      "Metrics:   episode_rewards 174.020000 epoch_idx 24.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.10batch/s]\n",
      "=>>>>>>>>>> EPOCH 25\n",
      "Metrics:   episode_rewards 188.830000 epoch_idx 25.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.02batch/s]\n",
      "=>>>>>>>>>> EPOCH 26\n",
      "Metrics:   episode_rewards 199.690000 epoch_idx 26.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.62batch/s]\n",
      "=>>>>>>>>>> EPOCH 27\n",
      "Metrics:   episode_rewards 210.320000 epoch_idx 27.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:08<00:00,  2.24batch/s]\n",
      "=>>>>>>>>>> EPOCH 28\n",
      "Metrics:   episode_rewards 215.000000 epoch_idx 28.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.79batch/s]\n",
      "=>>>>>>>>>> EPOCH 29\n",
      "Metrics:   episode_rewards 226.600000 epoch_idx 29.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.01batch/s]\n",
      "=>>>>>>>>>> EPOCH 30\n",
      "Metrics:   episode_rewards 240.450000 epoch_idx 30.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:04<00:00,  4.02batch/s]\n",
      "=>>>>>>>>>> EPOCH 31\n",
      "Metrics:   episode_rewards 255.330000 epoch_idx 31.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.68batch/s]\n",
      "=>>>>>>>>>> EPOCH 32\n",
      "Metrics:   episode_rewards 257.740000 epoch_idx 32.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.36batch/s]\n",
      "=>>>>>>>>>> EPOCH 33\n",
      "Metrics:   episode_rewards 277.680000 epoch_idx 33.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.52batch/s]\n",
      "=>>>>>>>>>> EPOCH 34\n",
      "Metrics:   episode_rewards 297.620000 epoch_idx 34.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.81batch/s]\n",
      "=>>>>>>>>>> EPOCH 35\n",
      "Metrics:   episode_rewards 317.560000 epoch_idx 35.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.80batch/s]\n",
      "=>>>>>>>>>> EPOCH 36\n",
      "Metrics:   episode_rewards 341.100000 epoch_idx 36.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.78batch/s]\n",
      "=>>>>>>>>>> EPOCH 37\n",
      "Metrics:   episode_rewards 358.080000 epoch_idx 37.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.74batch/s]\n",
      "=>>>>>>>>>> EPOCH 38\n",
      "Metrics:   episode_rewards 378.020000 epoch_idx 38.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.71batch/s]\n",
      "=>>>>>>>>>> EPOCH 39\n",
      "Metrics:   episode_rewards 397.960000 epoch_idx 39.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.66batch/s]\n",
      "=>>>>>>>>>> EPOCH 40\n",
      "Metrics:   episode_rewards 419.740000 epoch_idx 40.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.26batch/s]\n",
      "=>>>>>>>>>> EPOCH 41\n",
      "Metrics:   episode_rewards 433.680000 epoch_idx 41.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.06batch/s]\n",
      "=>>>>>>>>>> EPOCH 42\n",
      "Metrics:   episode_rewards 460.620000 epoch_idx 42.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.20batch/s]\n",
      "=>>>>>>>>>> EPOCH 43\n",
      "Metrics:   episode_rewards 480.520000 epoch_idx 43.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.52batch/s]\n",
      "=>>>>>>>>>> EPOCH 44\n",
      "Metrics:   episode_rewards 500.360000 epoch_idx 44.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.51batch/s]\n",
      "=>>>>>>>>>> EPOCH 45\n",
      "Metrics:   episode_rewards 518.710000 epoch_idx 45.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.47batch/s]\n",
      "=>>>>>>>>>> EPOCH 46\n",
      "Metrics:   episode_rewards 541.110000 epoch_idx 46.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.25batch/s]\n",
      "=>>>>>>>>>> EPOCH 47\n",
      "Metrics:   episode_rewards 559.660000 epoch_idx 47.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.41batch/s]\n",
      "=>>>>>>>>>> EPOCH 48\n",
      "Metrics:   episode_rewards 579.600000 epoch_idx 48.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.06batch/s]\n",
      "=>>>>>>>>>> EPOCH 49\n",
      "Metrics:   episode_rewards 596.680000 epoch_idx 49.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.83batch/s]\n",
      "=>>>>>>>>>> EPOCH 50\n",
      "Metrics:   episode_rewards 620.150000 epoch_idx 50.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.35batch/s]\n",
      "=>>>>>>>>>> EPOCH 51\n",
      "Metrics:   episode_rewards 639.330000 epoch_idx 51.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.34batch/s]\n",
      "=>>>>>>>>>> EPOCH 52\n",
      "Metrics:   episode_rewards 630.800000 epoch_idx 52.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:05<00:00,  3.34batch/s]\n",
      "=>>>>>>>>>> EPOCH 53\n",
      "Metrics:   episode_rewards 626.730000 epoch_idx 53.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.32batch/s]\n",
      "=>>>>>>>>>> EPOCH 54\n",
      "Metrics:   episode_rewards 628.560000 epoch_idx 54.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.29batch/s]\n",
      "=>>>>>>>>>> EPOCH 55\n",
      "Metrics:   episode_rewards 636.330000 epoch_idx 55.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.26batch/s]\n",
      "=>>>>>>>>>> EPOCH 56\n",
      "Metrics:   episode_rewards 641.540000 epoch_idx 56.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.26batch/s]\n",
      "=>>>>>>>>>> EPOCH 57\n",
      "Metrics:   episode_rewards 659.310000 epoch_idx 57.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.02batch/s]\n",
      "=>>>>>>>>>> EPOCH 58\n",
      "Metrics:   episode_rewards 659.310000 epoch_idx 58.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.64batch/s]\n",
      "=>>>>>>>>>> EPOCH 59\n",
      "Metrics:   episode_rewards 655.850000 epoch_idx 59.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:08<00:00,  2.50batch/s]\n",
      "=>>>>>>>>>> EPOCH 60\n",
      "Metrics:   episode_rewards 655.850000 epoch_idx 60.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.68batch/s]\n",
      "=>>>>>>>>>> EPOCH 61\n",
      "Metrics:   episode_rewards 649.040000 epoch_idx 61.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.10batch/s]\n",
      "=>>>>>>>>>> EPOCH 62\n",
      "Metrics:   episode_rewards 642.620000 epoch_idx 62.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.09batch/s]\n",
      "=>>>>>>>>>> EPOCH 63\n",
      "Metrics:   episode_rewards 642.620000 epoch_idx 63.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.07batch/s]\n",
      "=>>>>>>>>>> EPOCH 64\n",
      "Metrics:   episode_rewards 642.620000 epoch_idx 64.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.03batch/s]\n",
      "=>>>>>>>>>> EPOCH 65\n",
      "Metrics:   episode_rewards 633.400000 epoch_idx 65.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  3.03batch/s]\n",
      "=>>>>>>>>>> EPOCH 66\n",
      "Metrics:   episode_rewards 612.180000 epoch_idx 66.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  2.98batch/s]\n",
      "=>>>>>>>>>> EPOCH 67\n",
      "Metrics:   episode_rewards 629.890000 epoch_idx 67.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  2.96batch/s]\n",
      "=>>>>>>>>>> EPOCH 68\n",
      "Metrics:   episode_rewards 648.160000 epoch_idx 68.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  2.93batch/s]\n",
      "=>>>>>>>>>> EPOCH 69\n",
      "Metrics:   episode_rewards 654.930000 epoch_idx 69.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.84batch/s]\n",
      "=>>>>>>>>>> EPOCH 70\n",
      "Metrics:   episode_rewards 637.760000 epoch_idx 70.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:06<00:00,  2.87batch/s]\n",
      "=>>>>>>>>>> EPOCH 71\n",
      "Metrics:   episode_rewards 637.760000 epoch_idx 71.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.85batch/s]\n",
      "=>>>>>>>>>> EPOCH 72\n",
      "Metrics:   episode_rewards 618.170000 epoch_idx 72.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.82batch/s]\n",
      "=>>>>>>>>>> EPOCH 73\n",
      "Metrics:   episode_rewards 578.060000 epoch_idx 73.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.81batch/s]\n",
      "=>>>>>>>>>> EPOCH 74\n",
      "Metrics:   episode_rewards 568.860000 epoch_idx 74.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.79batch/s]\n",
      "=>>>>>>>>>> EPOCH 75\n",
      "Metrics:   episode_rewards 578.230000 epoch_idx 75.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.76batch/s]\n",
      "=>>>>>>>>>> EPOCH 76\n",
      "Metrics:   episode_rewards 579.700000 epoch_idx 76.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.72batch/s]\n",
      "=>>>>>>>>>> EPOCH 77\n",
      "Metrics:   episode_rewards 558.820000 epoch_idx 77.000000\n",
      "=>>>>>>>>>> DONE\n",
      "Training: 100%|██████████| 20/20 [00:07<00:00,  2.72batch/s]\n",
      "=>>>>>>>>>> EPOCH 78\n",
      "Metrics:   episode_rewards 569.490000 epoch_idx 78.000000\n",
      "=>>>>>>>>>> DONE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim\n",
    "\n",
    "from vel.api import TrainingInfo, EpochInfo\n",
    "from vel.rl.metrics import EpisodeRewardMetric\n",
    "from vel.storage.streaming.stdout import StdoutStreaming\n",
    "from vel.util.random import set_seed\n",
    "from vel.rl.env.mujoco import MujocoEnv\n",
    "from vel.rl.models.deterministic_policy_model import DeterministicPolicyModelFactory\n",
    "from vel.rl.models.backbone.mlp import MLPFactory\n",
    "from vel.rl.reinforcers.buffered_single_off_policy_iteration_reinforcer import (\n",
    "    BufferedSingleOffPolicyIterationReinforcer, BufferedSingleOffPolicyIterationReinforcerSettings\n",
    ")\n",
    "from vel.rl.algo.policy_gradient.ddpg import DeepDeterministicPolicyGradient\n",
    "from vel.rl.env_roller.single.deque_replay_roller_ou_noise import DequeReplayRollerOuNoise\n",
    "from vel.optimizers.adam import AdamFactory\n",
    "\n",
    "\n",
    "device = torch.device('cpu:0')\n",
    "seed = 1002\n",
    "\n",
    "# Set random seed in python std lib, numpy and pytorch\n",
    "set_seed(seed)\n",
    "\n",
    "env = MujocoEnv('InvertedPendulum-v2').instantiate(seed=seed)\n",
    "\n",
    "model_factory = DeterministicPolicyModelFactory(\n",
    "    policy_backbone=MLPFactory(input_length=4, hidden_layers=[64, 64], activation='tanh'),\n",
    "    value_backbone=MLPFactory(input_length=5, hidden_layers=[64, 64], activation='tanh'),\n",
    ")\n",
    "\n",
    "model = model_factory.instantiate(action_space=env.action_space)\n",
    "\n",
    "reinforcer = BufferedSingleOffPolicyIterationReinforcer(\n",
    "    device=device,\n",
    "    settings=BufferedSingleOffPolicyIterationReinforcerSettings(\n",
    "        batch_rollout_rounds=100,\n",
    "        batch_training_rounds=50,\n",
    "        batch_size=64,\n",
    "        discount_factor=0.99\n",
    "    ),\n",
    "    environment=env,\n",
    "    model=model,\n",
    "    algo=DeepDeterministicPolicyGradient(\n",
    "        model_factory=model_factory,\n",
    "        tau=0.01,\n",
    "    ),\n",
    "    env_roller=DequeReplayRollerOuNoise(\n",
    "        environment=env,\n",
    "        device=device,\n",
    "        batch_size=64,\n",
    "        buffer_capacity=1_000_000,\n",
    "        buffer_initial_size=2_000,\n",
    "        noise_std_dev=0.2,\n",
    "        normalize_observations=True,\n",
    "        normalize_returns=True,\n",
    "        discount_factor=0.99\n",
    "    )\n",
    ")\n",
    "\n",
    "# Optimizer helper - A weird regularization settings I've copied from OpenAI code\n",
    "adam_optimizer = AdamFactory(\n",
    "    lr=[1.0e-4, 1.0e-3, 1.0e-3],\n",
    "    weight_decay=[0.0, 0.0, 0.001],\n",
    "    eps=1.0e-4,\n",
    "    layer_groups=True\n",
    ").instantiate(model)\n",
    "\n",
    "# Overall information store for training information\n",
    "training_info = TrainingInfo(\n",
    "    metrics=[\n",
    "        EpisodeRewardMetric('episode_rewards'),  # Calculate average reward from episode\n",
    "    ],\n",
    "    callbacks=[StdoutStreaming()]  # Print live metrics every epoch to standard output\n",
    ")\n",
    "\n",
    "# A bit of training initialization bookkeeping...\n",
    "training_info.initialize()\n",
    "reinforcer.initialize_training(training_info)\n",
    "training_info.on_train_begin()\n",
    "\n",
    "# Let's make 20 batches per epoch to average metrics nicely\n",
    "num_epochs = int(1.0e5 / 64 / 20)\n",
    "\n",
    "# Normal handrolled training loop\n",
    "for i in range(1, num_epochs+1):\n",
    "    epoch_info = EpochInfo(\n",
    "        training_info=training_info,\n",
    "        global_epoch_idx=i,\n",
    "        batches_per_epoch=20,\n",
    "        optimizer=adam_optimizer\n",
    "    )\n",
    "\n",
    "    reinforcer.train_epoch(epoch_info)\n",
    "\n",
    "training_info.on_train_end()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy_backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ],
     "output_type": "error"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sgillen/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#mj_env = env.env.env # lol don't even ask\n",
    "ob = env.reset()     \n",
    "\n",
    "while True:\n",
    "    action = model.action(torch.Tensor(ob)).item()\n",
    "    ob, _, done, _ =  env.step(action)\n",
    "    #if reward == 1:\n",
    "    #    print(\"balanced\")\n",
    "    env.render()\n",
    "    if done:\n",
    "        ob = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
