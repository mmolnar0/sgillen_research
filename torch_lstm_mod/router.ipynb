{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "# for the training only\n",
    "from itertools import count\n",
    "import gym\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "class Router(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, router_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Routing layer gates\n",
    "        self.r_linear1 = nn.Linear(input_size, router_size)\n",
    "        self.r_linear2 = nn.Linear(router_size, 2)\n",
    "        \n",
    "        # Swingup layer gates\n",
    "        self.s_linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.s_linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # This is basically our static gain matrix (maybe I should make this a matrix rather than a linear layer...)\n",
    "        self.k = nn.Linear(input_size,1,bias=False) \n",
    "        \n",
    "        # Required for the training\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Gating\n",
    "        g = torch.sigmoid(self.r_linear1(x))\n",
    "        g = torch.sigmoid(self.r_linear2(g))\n",
    "        d = softmax(g, dim=0)\n",
    "        \n",
    "        # Swingup\n",
    "        s = torch.relu(self.s_linear1(x))\n",
    "        ys = self.s_linear2(s)\n",
    "        \n",
    "        # Balance\n",
    "        yb = self.k(x)\n",
    "    \n",
    "        return ys, yb, d\n",
    "    \n",
    "    \n",
    "def select_action(x, policy):\n",
    "        x = torch.from_numpy(x).float().unsqueeze(0)\n",
    "\n",
    "        ys, yb, d = policy(x)\n",
    "        m = torch.distributions.Categorical(d)\n",
    "        path = m.sample()\n",
    "        \n",
    "        policy.saved_log_probs.append(m.log_prob(path))\n",
    "        \n",
    "        if path.item() == 0:\n",
    "            return ys.item()\n",
    "        else:\n",
    "            return yb.item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# Calculates the time weighted rewards, policy losses, and optimizers\n",
    "def finish_episode(policy):\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    rewards = []\n",
    "    \n",
    "    gamma = .5\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma*R\n",
    "        rewards.append(R)\n",
    "        \n",
    "    rewards = rewards[::-1]\n",
    "    rewards = torch.tensor(rewards)\n",
    "    rewards = (rewards - rewards.mean())/(rewards.std() + eps)\n",
    "    \n",
    "    for log_prob, reward in zip(policy.saved_log_probs, rewards):\n",
    "        policy_loss.append(-log_prob * reward)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = Router(2,4,4)\n",
    "#(ys, yb, d) = net(torch.randn(2))\n",
    "#print(\"g1: \", g1)\n",
    "#print(\"g2: \", g2)\n",
    "#print(\"d: \", d)\n",
    "#print(\"x:\", x)\n",
    "#print()\n",
    "#print(\"G1\", net.r_linear1)\n",
    "#print(\"G2\", net.r_linear2)\n",
    "\n",
    "\n",
    "#action = select_action(np.random.randn(2),net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_step(self,u):\n",
    "    th, thdot = self.state # th := theta\n",
    "\n",
    "    g = 10.\n",
    "    m = 1.\n",
    "    l = 1.\n",
    "    dt = self.dt\n",
    "\n",
    "    u = np.clip(u, -self.max_torque, self.max_torque)\n",
    "    self.last_u = u # for rendering\n",
    "    costs = angle_normalize(th)**2 + .1*thdot**2 + .001*(u**2)\n",
    "\n",
    "    newthdot = thdot + (-3*g/(2*l) * np.sin(th + np.pi) + 3./(m*l**2)*u) * dt\n",
    "    newth = th + newthdot*dt\n",
    "    newthdot = np.clip(newthdot, -self.max_speed, self.max_speed) #pylint: disable=E1111\n",
    "\n",
    "    self.state = np.array([newth, newthdot])\n",
    "    return self._get_obs(), -costs, False, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8faff95c421f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pendulum-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#env.step = fixed_step.__get__(env, gym.Env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#env.seed(args.seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#torch.manual_seed(args.seed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "#env.step = fixed_step.__get__(env, gym.Env)\n",
    "#env.seed(args.seed)\n",
    "#torch.manual_seed(args.seed)\n",
    "\n",
    "policy = Router(3,8,5)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "\n",
    "running_reward = 10\n",
    "for i_episode in count(1):\n",
    "    state = env.reset()\n",
    "    for t in range(10000):\n",
    "        action = select_action(state, policy)\n",
    "        state,reward, done, _ = env.step(np.array([action, 0]))\n",
    "        \n",
    "        policy.rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        running_reward = running_reward * 0.99 + t * 0.01\n",
    "        finish_episode(policy) \n",
    "        \n",
    "        \n",
    "        log_interval = 10\n",
    "        \n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}'.format(\n",
    "                i_episode, t, running_reward))\n",
    "        #if running_reward > env.spec.reward_threshold:\n",
    "        #    print(\"Solved! Running reward is now {} and \"\n",
    "        #          \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
